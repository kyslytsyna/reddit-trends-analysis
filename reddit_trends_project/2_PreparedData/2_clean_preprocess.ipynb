{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1129f2b8-99a6-42f4-8fa8-c16f63722285",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365e6b4a-3b2c-45ae-863d-c9afc57b1949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe Biden elected president of the United States</td>\n",
       "      <td>365122</td>\n",
       "      <td>https://apnews.com/article/election-2020-joe-b...</td>\n",
       "      <td>1.604767e+09</td>\n",
       "      <td>28194</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chauvin found guilty of murder, manslaughter i...</td>\n",
       "      <td>250268</td>\n",
       "      <td>https://kstp.com/news/former-minneapolis-polic...</td>\n",
       "      <td>1.618953e+09</td>\n",
       "      <td>27550</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>President Donald Trump says he has tested posi...</td>\n",
       "      <td>233319</td>\n",
       "      <td>https://www.cnbc.com/2020/10/02/president-dona...</td>\n",
       "      <td>1.601615e+09</td>\n",
       "      <td>33133</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blizzard Employees Staged a Walkout After the ...</td>\n",
       "      <td>226328</td>\n",
       "      <td>https://www.thedailybeast.com/blizzard-employe...</td>\n",
       "      <td>1.570654e+09</td>\n",
       "      <td>9392</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump has left the White House for the last ti...</td>\n",
       "      <td>222636</td>\n",
       "      <td>https://edition.cnn.com/politics/live-news/bid...</td>\n",
       "      <td>1.611149e+09</td>\n",
       "      <td>11623</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   score  \\\n",
       "0   Joe Biden elected president of the United States  365122   \n",
       "1  Chauvin found guilty of murder, manslaughter i...  250268   \n",
       "2  President Donald Trump says he has tested posi...  233319   \n",
       "3  Blizzard Employees Staged a Walkout After the ...  226328   \n",
       "4  Trump has left the White House for the last ti...  222636   \n",
       "\n",
       "                                                 url   created_utc  \\\n",
       "0  https://apnews.com/article/election-2020-joe-b...  1.604767e+09   \n",
       "1  https://kstp.com/news/former-minneapolis-polic...  1.618953e+09   \n",
       "2  https://www.cnbc.com/2020/10/02/president-dona...  1.601615e+09   \n",
       "3  https://www.thedailybeast.com/blizzard-employe...  1.570654e+09   \n",
       "4  https://edition.cnn.com/politics/live-news/bid...  1.611149e+09   \n",
       "\n",
       "   num_comments subreddit  \n",
       "0         28194      news  \n",
       "1         27550      news  \n",
       "2         33133      news  \n",
       "3          9392      news  \n",
       "4         11623      news  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the collected data \n",
    "df_news = pd.read_csv(\"../1_OriginalData/news_data.csv\")\n",
    "df_worldnews = pd.read_csv(\"../1_OriginalData/worldnews_data.csv\")\n",
    "df_politics = pd.read_csv(\"../1_OriginalData/politics_data.csv\")\n",
    "df_technology = pd.read_csv(\"../1_OriginalData/technology_data.csv\")\n",
    "df_worldpolitics = pd.read_csv(\"../1_OriginalData/worldpolitics_data.csv\")\n",
    "df_TrueReddit = pd.read_csv(\"../1_OriginalData/TrueReddit_data.csv\")\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "df = pd.concat([df_news, df_worldnews, df_politics, df_technology, df_worldpolitics, df_TrueReddit], ignore_index=True)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aec7f2-9809-4446-a131-aba770e2271e",
   "metadata": {},
   "source": [
    "### Clean and Keep Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c10d210-707e-49e9-a7e2-6a3c8560abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['title', 'score', 'url', 'created_utc', 'num_comments', 'subreddit'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34974e5d-22ab-4047-ae67-1a94127762b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "title           0\n",
      "score           0\n",
      "num_comments    0\n",
      "created_utc     0\n",
      "subreddit       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Keep only the relevant ones\n",
    "columns_to_keep = ['title', 'score', 'num_comments', 'created_utc', 'subreddit']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_counts = df.isna().sum()\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c97c00-ee42-4800-8fcb-6164c2879617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate titles within subreddits: 23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>Anne Frank and her family were also denied ent...</td>\n",
       "      <td>3912</td>\n",
       "      <td>341</td>\n",
       "      <td>1.485631e+09</td>\n",
       "      <td>TrueReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>Anne Frank and her family were also denied ent...</td>\n",
       "      <td>2518</td>\n",
       "      <td>396</td>\n",
       "      <td>1.448467e+09</td>\n",
       "      <td>TrueReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>Study Reveals It Costs Less to Give the Homele...</td>\n",
       "      <td>4107</td>\n",
       "      <td>731</td>\n",
       "      <td>1.404391e+09</td>\n",
       "      <td>TrueReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>Study Reveals It Costs Less to Give the Homele...</td>\n",
       "      <td>2697</td>\n",
       "      <td>194</td>\n",
       "      <td>1.596846e+09</td>\n",
       "      <td>TrueReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Georgia Judge Throws Out Trump Campaign Lawsui...</td>\n",
       "      <td>106393</td>\n",
       "      <td>2597</td>\n",
       "      <td>1.604594e+09</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>Georgia Judge Throws Out Trump Campaign Lawsui...</td>\n",
       "      <td>97311</td>\n",
       "      <td>4093</td>\n",
       "      <td>1.604872e+09</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>Warren reintroduces bill to bar lawmakers from...</td>\n",
       "      <td>101877</td>\n",
       "      <td>2341</td>\n",
       "      <td>1.608388e+09</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>Warren reintroduces bill to bar lawmakers from...</td>\n",
       "      <td>89617</td>\n",
       "      <td>1709</td>\n",
       "      <td>1.609092e+09</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>I know you’re tired of hearing about net neutr...</td>\n",
       "      <td>74764</td>\n",
       "      <td>1690</td>\n",
       "      <td>1.525528e+09</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>I know you’re tired of hearing about net neutr...</td>\n",
       "      <td>53856</td>\n",
       "      <td>1256</td>\n",
       "      <td>1.526400e+09</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title   score  num_comments  \\\n",
       "2562  Anne Frank and her family were also denied ent...    3912           341   \n",
       "2732  Anne Frank and her family were also denied ent...    2518           396   \n",
       "2553  Study Reveals It Costs Less to Give the Homele...    4107           731   \n",
       "2700  Study Reveals It Costs Less to Give the Homele...    2697           194   \n",
       "1046  Georgia Judge Throws Out Trump Campaign Lawsui...  106393          2597   \n",
       "1093  Georgia Judge Throws Out Trump Campaign Lawsui...   97311          4093   \n",
       "1060  Warren reintroduces bill to bar lawmakers from...  101877          2341   \n",
       "1170  Warren reintroduces bill to bar lawmakers from...   89617          1709   \n",
       "1609  I know you’re tired of hearing about net neutr...   74764          1690   \n",
       "1911  I know you’re tired of hearing about net neutr...   53856          1256   \n",
       "\n",
       "       created_utc   subreddit  \n",
       "2562  1.485631e+09  TrueReddit  \n",
       "2732  1.448467e+09  TrueReddit  \n",
       "2553  1.404391e+09  TrueReddit  \n",
       "2700  1.596846e+09  TrueReddit  \n",
       "1046  1.604594e+09    politics  \n",
       "1093  1.604872e+09    politics  \n",
       "1060  1.608388e+09    politics  \n",
       "1170  1.609092e+09    politics  \n",
       "1609  1.525528e+09  technology  \n",
       "1911  1.526400e+09  technology  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by subreddit and find duplicated titles\n",
    "duplicates = df[df.duplicated(subset=['subreddit', 'title'], keep=False)]\n",
    "\n",
    "# Sort for readability\n",
    "duplicates = duplicates.sort_values(by=['subreddit', 'title'])\n",
    "\n",
    "print(f\"Number of duplicate titles within subreddits: {len(duplicates)}\")\n",
    "duplicates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788fa276-87c4-41cb-ae3e-41384c7df358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate titles within the same subreddit, keeping the first one\n",
    "df = df.drop_duplicates(subset=['subreddit', 'title'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58448279-04d5-4812-9f6c-de680c190712",
   "metadata": {},
   "source": [
    "### Convert Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8740013-9a9a-4691-9291-59b07d3b7c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe Biden elected president of the United States</td>\n",
       "      <td>365122</td>\n",
       "      <td>28194</td>\n",
       "      <td>1.604767e+09</td>\n",
       "      <td>news</td>\n",
       "      <td>2020-11-07 16:28:37</td>\n",
       "      <td>2020-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chauvin found guilty of murder, manslaughter i...</td>\n",
       "      <td>250268</td>\n",
       "      <td>27550</td>\n",
       "      <td>1.618953e+09</td>\n",
       "      <td>news</td>\n",
       "      <td>2021-04-20 21:07:44</td>\n",
       "      <td>2021-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>President Donald Trump says he has tested posi...</td>\n",
       "      <td>233319</td>\n",
       "      <td>33133</td>\n",
       "      <td>1.601615e+09</td>\n",
       "      <td>news</td>\n",
       "      <td>2020-10-02 05:04:17</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blizzard Employees Staged a Walkout After the ...</td>\n",
       "      <td>226328</td>\n",
       "      <td>9392</td>\n",
       "      <td>1.570654e+09</td>\n",
       "      <td>news</td>\n",
       "      <td>2019-10-09 20:45:17</td>\n",
       "      <td>2019-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump has left the White House for the last ti...</td>\n",
       "      <td>222636</td>\n",
       "      <td>11623</td>\n",
       "      <td>1.611149e+09</td>\n",
       "      <td>news</td>\n",
       "      <td>2021-01-20 13:16:44</td>\n",
       "      <td>2021-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   score  num_comments  \\\n",
       "0   Joe Biden elected president of the United States  365122         28194   \n",
       "1  Chauvin found guilty of murder, manslaughter i...  250268         27550   \n",
       "2  President Donald Trump says he has tested posi...  233319         33133   \n",
       "3  Blizzard Employees Staged a Walkout After the ...  226328          9392   \n",
       "4  Trump has left the White House for the last ti...  222636         11623   \n",
       "\n",
       "    created_utc subreddit        created_date year_month  \n",
       "0  1.604767e+09      news 2020-11-07 16:28:37    2020-11  \n",
       "1  1.618953e+09      news 2021-04-20 21:07:44    2021-04  \n",
       "2  1.601615e+09      news 2020-10-02 05:04:17    2020-10  \n",
       "3  1.570654e+09      news 2019-10-09 20:45:17    2019-10  \n",
       "4  1.611149e+09      news 2021-01-20 13:16:44    2021-01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'created_utc' to datetime\n",
    "df['created_date'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Extract year and month for time-based grouping\n",
    "df['year_month'] = df['created_date'].dt.to_period('M')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852ee2b-6f49-41f9-a8d5-936db849dbe4",
   "metadata": {},
   "source": [
    "### Basic Text Cleaning for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8410314-c077-417c-b817-7226629cbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleanup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)             # remove URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)            # remove non-letter characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()        # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "df['clean_title'] = df['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "534b62fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-string clean_title rows dropped: 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity-check, drop corrupted rows\n",
    "bad_rows = df[~df[\"clean_title\"].apply(lambda x: isinstance(x, str))]\n",
    "print(f\"Non-string clean_title rows dropped: {len(bad_rows)}\")\n",
    "\n",
    "df = df[df[\"clean_title\"].apply(lambda x: isinstance(x, str))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a79ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marynakyslytsyna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and stopword removal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "BASE_STOP   = set(stopwords.words('english'))\n",
    "CUSTOM_STOP = {\n",
    "    'schlop','upvote','fuck','says','dont','get','like','one','us','man',\n",
    "    'people','years','year','america','white','black',\n",
    "    'make','take','know','think','going','want'\n",
    "}\n",
    "\n",
    "def clean_and_tokenize(text: str) -> list[str]:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n",
    "    return [\n",
    "        w for w in words\n",
    "        if w not in BASE_STOP and w not in CUSTOM_STOP\n",
    "    ]\n",
    "\n",
    "# Create tokens column and remove any non-string leftovers\n",
    "df['tokens'] = (\n",
    "    df['clean_title']\n",
    "        .apply(clean_and_tokenize)\n",
    "        .apply(lambda lst: [w for w in lst if isinstance(w, str)])\n",
    ")\n",
    "\n",
    "# Remove rows with no usable tokens\n",
    "df = df[df['tokens'].str.len() > 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317da93e-91df-4746-b8ea-1d6016948674",
   "metadata": {},
   "source": [
    "### Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8907798-1543-4aca-a55c-3f523fcc9563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"cleaned_reddit_posts.csv\", index=False)\n",
    "print(\"Cleaned data saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
